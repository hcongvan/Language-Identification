{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>Function replace some symbol</h2>\n",
    "Follow table below we replace some character to 1 symbol, because all of them resentation the popular word, such as english token [a-zA-Z] \n",
    "<img src=\"img/table1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNoise(data):\n",
    "    data['text'] = data['text'].str.replace('[=<>\"[]{}/:-;.,\\'\\(\\)%]', ' ')\n",
    "    data['text'] = data['text'].str.replace('[0-9]', ' ')\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>KullBack-Leibler Divergence</h2>\n",
    "\n",
    "- Make sure 2 params are array and same length\n",
    "- Normalize them \n",
    "\n",
    "<img src=\"img/kl.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdivergence(p,q):\n",
    "    if len(p) != len(q):\n",
    "        return False\n",
    "    p = p/np.sum(p)\n",
    "    q = q/np.sum(q)\n",
    "    return np.sum(p*np.log(p/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>Get data train</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>language</th>\n      <th>text</th>\n      <th>length_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bg</td>\n      <td>Състав на Парламента: вж. протоколиОдобряване ...</td>\n      <td>327263</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cs</td>\n      <td>Schválení zápisu z předchozího zasedání: viz z...</td>\n      <td>317927</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>da</td>\n      <td>Genoptagelse af sessionenJeg erklærer Europa-P...</td>\n      <td>678400</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>de</td>\n      <td>Wiederaufnahme der SitzungsperiodeIch erkläre ...</td>\n      <td>747690</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>el</td>\n      <td>Επαvάληψη της συvσδoυΚηρύσσω την επανάληψη της...</td>\n      <td>523277</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  language                                               text  length_text\n0       bg  Състав на Парламента: вж. протоколиОдобряване ...       327263\n1       cs  Schválení zápisu z předchozího zasedání: viz z...       317927\n2       da  Genoptagelse af sessionenJeg erklærer Europa-P...       678400\n3       de  Wiederaufnahme der SitzungsperiodeIch erkläre ...       747690\n4       el  Επαvάληψη της συvσδoυΚηρύσσω την επανάληψη της...       523277"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('formatted_data.csv', sep=';', error_bad_lines=False)\n",
    "data = removeNoise(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>Get data test from external test set</h2>\n",
    "With data test:\n",
    "\n",
    "- Using other source but keep format and language support\n",
    "- Select random line for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>language</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16130</th>\n      <td>pt</td>\n      <td>Mas não basta que os países tenham assinado es...</td>\n    </tr>\n    <tr>\n      <th>4590</th>\n      <td>el</td>\n      <td>Ο συνάδελφός μου Βουλευτής κ. Öger ανέφερε μόλ...</td>\n    </tr>\n    <tr>\n      <th>10555</th>\n      <td>hu</td>\n      <td>Nem helyes, hogy a projekt elkészítése a part ...</td>\n    </tr>\n    <tr>\n      <th>17240</th>\n      <td>ro</td>\n      <td>Prin urmare, avem nevoie de un angajament clar...</td>\n    </tr>\n    <tr>\n      <th>17516</th>\n      <td>ro</td>\n      <td>În primul rând, probabil deoarece, cu nivelul ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      language                                               text\n16130       pt  Mas não basta que os países tenham assinado es...\n4590        el  Ο συνάδελφός μου Βουλευτής κ. Öger ανέφερε μόλ...\n10555       hu  Nem helyes, hogy a projekt elkészítése a part ...\n17240       ro  Prin urmare, avem nevoie de un angajament clar...\n17516       ro  În primul rând, probabil deoarece, cu nivelul ..."
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('europarl.csv',sep=';', error_bad_lines=False)\n",
    "test = removeNoise(test)\n",
    "test= test.reindex(np.random.permutation(test.index))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>Prepair data</h2>\n",
    "\n",
    "Transform data and test set to numpy array with : \n",
    "- n: length of data train\n",
    "- m: length of data test\n",
    "\n",
    "List of all the languages whose detection is supported:\n",
    "\n",
    "- 'bg': Bulgarian\n",
    "- 'cs': Czech\n",
    "- 'da': Danish\n",
    "- 'de': German\n",
    "- 'el': Greek, Modern\n",
    "- 'en': English\n",
    "- 'es': Spanish\n",
    "- 'et': Estonian\n",
    "- 'fi': Finnish\n",
    "- 'fr': French\n",
    "- 'hu': Hungarian\n",
    "- 'it': Italian\n",
    "- 'lt': Lithuanian\n",
    "- 'lv': Latvian\n",
    "- 'nl': Dutch\n",
    "- 'pl': Polish\n",
    "- 'pt': Portuguese\n",
    "- 'ro': Romanian\n",
    "- 'sk': Slovak\n",
    "- 'sl': Slovenian\n",
    "- 'sv': Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values\n",
    "test = test.values\n",
    "\n",
    "n = data.shape[0]\n",
    "m = test.shape[0]\n",
    "\n",
    "labels = data[:,0]\n",
    "labels_test = test[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepair data train:\n",
    "- using CountVectorizer to get all token from raw text\n",
    "- Seperate each token language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_train = []\n",
    "for i,v in enumerate(data):\n",
    "    vector = CountVectorizer(analyzer='char',encoding='latin-1',ngram_range=(2,2))\n",
    "    y = vector.fit_transform([v[1].replace(' ','')])\n",
    "    language_train.append(np.array([vector,y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict task:\n",
    "\n",
    "- Get each text from test data\n",
    "- Get token don't exit from _p with _q assign to _set(p is set token of test, and q is set token of train)\n",
    "<img src=\"img/t_cup.png\"/>\n",
    "- If _set empty => _p inside _q then calculate KL-divergence D(_p||_q) will > 0\n",
    "<img src=\"img/t_cap.png\"/>\n",
    "- Else _set have some character => _p overlap _q the D(_p||_q) will Infinity\n",
    "- Store D(_p||_q) into t array\n",
    "- After run all language support, predict = argmin(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = []\n",
    "true_position = 0\n",
    "for i,(_,data_test) in enumerate(test):\n",
    "    vector_test = TfidfVectorizer(analyzer='char',encoding='latin-1',ngram_range=(2,2))\n",
    "    transform = vector_test.fit_transform([data_test.replace(' ','')])\n",
    "    t = np.array([float('Inf')]*n)\n",
    "    for j,(vector_train,transform_y) in enumerate(language_train):\n",
    "        _tmp = set(vector_train.vocabulary_)^set(vector_test.vocabulary_)\n",
    "        _set = list((set(_tmp)|set(vector_test.vocabulary_))^ set(vector_train.vocabulary_))\n",
    "        if not bool(_set):\n",
    "            k = len(vector_train.vocabulary_)\n",
    "            _q = np.zeros(k)\n",
    "            _q[transform_y.indices] = transform_y.data\n",
    "            _p = np.zeros(k)\n",
    "            _p[transform.indices] = transform.data\n",
    "            t[j] = scipy.stats.entropy(_p,_q)\n",
    "    predict_label.append(labels[np.argmin(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "precision    recall  f1-score   support\n\n          bg       0.21      1.00      0.35       986\n          cs       0.94      0.75      0.84       991\n          da       0.89      0.73      0.80       991\n          de       0.95      0.58      0.72       989\n          el       1.00      0.75      0.86       963\n          en       0.95      0.08      0.14       972\n          es       0.99      0.34      0.51       979\n          et       0.83      0.72      0.77       985\n          fi       0.98      0.66      0.79       989\n          fr       0.60      0.09      0.16       979\n          hu       0.69      0.84      0.75       978\n          it       0.96      0.16      0.28       965\n          lt       0.47      0.85      0.61       982\n          lv       0.86      0.85      0.85       961\n          nl       0.73      0.21      0.33       984\n          pl       0.85      0.82      0.83       983\n          pt       0.74      0.75      0.75       983\n          ro       0.90      0.86      0.88       916\n          sk       0.67      0.85      0.75       922\n          sl       0.72      0.60      0.65       980\n          sv       0.52      0.85      0.64       995\n\n   micro avg       0.63      0.63      0.63     20473\n   macro avg       0.78      0.63      0.63     20473\nweighted avg       0.78      0.63      0.63     20473\n\n0.6340546085087676\n"
    }
   ],
   "source": [
    "print(classification_report(labels_test, predict_label))            \n",
    "print (accuracy_score(labels_test, predict_label))"
   ]
  }
 ]
}